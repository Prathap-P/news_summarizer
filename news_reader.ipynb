{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1fc8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from main import read_website_content\n",
    "from langchain_groq import ChatGroq\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf38eeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da44e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_llm = ChatGroq(\n",
    "    model= \"openai/gpt-oss-20b\",\n",
    "    api_key= os.getenv(\"GROQ_API_KEY\"),\n",
    "    temperature= 0.3,\n",
    "    max_tokens= 1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ad5b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = {\n",
    "    \"system\": \"You are a helpful assistant that summarizes news articles.\",\n",
    "    \"user\": \"Summarize the following article content:\\n\\n{content}\",\n",
    "}\n",
    "\n",
    "def summarize_article(content):\n",
    "    response = groq_llm.invoke(content)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68c2ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summarize_article(f\"\"\"\n",
    "                            You are a helpful assistant that summarizes news articles.\n",
    "                            Summarize the following article content:{read_website_content([\"https://www.bbc.com/future/article/20241122-ai-deepfakes-is-there-something-special-about-the-human-voice\"])[0].page_content}\n",
    "                            \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a195be",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_website_content(\"https://www.bbc.com/future/article/20241122-ai-deepfakes-is-there-something-special-about-the-human-voice\")[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a650c028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.callbacks import get_openai_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976b0683",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_news_explainer = \"\"\"\n",
    "You are a technical news explainer.\n",
    "\n",
    "You will receive the full content of a technical news article (technology, AI, software, hardware, cloud, cybersecurity, science, engineering).\n",
    "\n",
    "Your responsibilities:\n",
    "\n",
    "1) Explain the article\n",
    "\n",
    "Explain what happened, why it matters, and how it works\n",
    "\n",
    "Break down technical concepts, jargon, and acronyms\n",
    "\n",
    "Add necessary background for understanding\n",
    "\n",
    "Assume the user is technical but may not know this specific domain\n",
    "\n",
    "2) Summarize on request\n",
    "\n",
    "If asked to summarize:\n",
    "\n",
    "Provide a concise summary by default\n",
    "\n",
    "Provide a detailed/technical summary if explicitly requested\n",
    "\n",
    "Do not add opinions unless asked\n",
    "\n",
    "3) Answer follow-up questions\n",
    "\n",
    "Maintain full conversational context\n",
    "\n",
    "Answer using:\n",
    "\n",
    "Information from the article\n",
    "\n",
    "Logical technical inference\n",
    "\n",
    "If something is unclear or not stated, say so explicitly\n",
    "\n",
    "4) Technical depth control\n",
    "\n",
    "Adjust depth based on the question:\n",
    "\n",
    "“Explain” → conceptual\n",
    "\n",
    "“How does it work?” → technical mechanism\n",
    "\n",
    "“Why?” → trade-offs and reasoning\n",
    "\n",
    "“Implications” → engineering, business, or ecosystem impact\n",
    "\n",
    "Rules\n",
    "\n",
    "Be precise and factual\n",
    "\n",
    "Do not hallucinate\n",
    "\n",
    "Clearly separate facts vs inference\n",
    "\n",
    "Avoid marketing language\n",
    "\n",
    "Use bullets, diagrams-in-text, or stepwise explanations when helpful\n",
    "\n",
    "Output style\n",
    "\n",
    "Clear, structured, and technical\n",
    "\n",
    "Concise but complete\n",
    "\n",
    "No emojis\n",
    "\n",
    "No fluff\n",
    "\n",
    "Initial response behavior\n",
    "\n",
    "After the article is provided:\n",
    "\n",
    "Confirm understanding\n",
    "\n",
    "Give a high-level technical explanation\n",
    "\n",
    "Offer next actions:\n",
    "\n",
    "Summary\n",
    "\n",
    "Deep dive\n",
    "\n",
    "Architecture explanation\n",
    "\n",
    "Pros/cons\n",
    "\n",
    "Implications\n",
    "\n",
    "Follow-up questions\"\"\"\n",
    "\n",
    "\n",
    "prompt_template = PromptTemplate(input_variables = [\"history\", \"input\"], template = f\"\"\"\n",
    "    {system_message_news_explainer}\n",
    "    Conversation History:\n",
    "    {{history}}\n",
    "    User Question:\n",
    "    {{input}}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47553c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_memory_100 = ConversationBufferWindowMemory(k=100)\n",
    "\n",
    "conversation_chain = ConversationChain(\n",
    "    llm = groq_llm,\n",
    "    memory = window_memory_100,\n",
    "    prompt = prompt_template,\n",
    "    verbose = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2cb47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_chain.memory.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36201be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    input_text = input(\"Input: \")\n",
    "    if input_text.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "    \n",
    "    response = conversation_chain.invoke({\n",
    "        \"input\": input_text\n",
    "    })\n",
    "    \n",
    "    display(Markdown(response['response']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
